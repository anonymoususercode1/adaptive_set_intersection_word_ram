# Adaptive Set Intersection Word RAM
--------
## Building the code
This repository contains implementations of adaptive data structures and algorithms, based on compressed tries, designed for efficient set intersection. These implementations support the results presented in the paper "Adaptive Set Intersection on the Word RAM" [cite the paper].
The code was tested in ubuntu 22.04.5, cmake 3.22.1 and gcc 11.4.0 version. The code has dependencies on the [**sdsl**](https://github.com/simongog/sdsl-lite) library.

To build the code in linux sistems, run the following commands:

    mkdir build
    cd build
    cmake ..
    make

Now, all executables are in **build** folder.

## Input data format
Our implementation only need the collection of docID's (posting lists) following format of [**dsi2**](https://github.com/ot/ds2i) and [**pisa**](https://github.com/pisa-engine/pisa) projects, the posting lists are written as 32-bit little-endian unsigned integers. The files containing the collections must start with a singleton binary sequence, where it's only integer is the number of documents of collection or the universe (1 u). It is then followed by one binary sequence for each posting list. 

## How to compress collection
For compress a collection need to use build_("variant") exec as following. The "variant" value must be replaced with the data structure variant you want to use, for example, wBtrie (**w BTRIE (1)**), x2WRBtrie (**w BTRIE (2)**), x3WRBtrie (**w BTRIE (3)**), or x3WTRBtrie (**TOP w BTRIE (2)**).

    ./tools/build_("variant") [collection_file_name] [--rank rank_type] [--out o] [--min_size m]
Where:
* --rank rank_type (required): its the type of rank data structure, the posible values are: v, v5.
* --out o (optional): name of output file to save the tries, if not specified only return the space metrics of collection.
* --min_size m (optional): filter lists of length less than a m.

## Intersection
For test the intersection of the tries, only need the file containing the collection compressed and a file with lists of sets involved in a query. The each line of queries file contain the id's of sets involved in a intersection to compute, every id it's separated by only one space. Every id of set correspond position of set in the complete collection.

An example to calculate the intersection in a collection using a query file is the following:

    ./tools/intersection_querylog_("variant") [collection_file_name] [queries_file_name] [--parallel t/f]
    
Where:
* collection_file_name: The path to the file containing the compressed collection. This is the file generated by the `build_` command, using the `--out o` parameter.

* queries_file_name: The path to the text file with the queries. Each line contains the IDs of the sets to be intersected, separated by a space.

* --parallel t/f (optional): Controls parallel execution. Use `t` to activate execution using all available processor threads. If you omit this parameter or set it to `f` (false), the execution will be sequential.
    
## Replicate Results
If you want replicate the results of paper, yo can download CC-News inverted index already processed by Mackenzie et al. in CIFF format from here: <http://go.unimelb.edu.au/u3nj>, and you can transform the index from CIFF to PISA format using <https://github.com/pisa-engine/ciff>.

## Reproducing Results for Artifact Evaluation (AE)
To reproduce the paper's experiments, follow these two steps:

* Download the Data
Get all the datasets from this link: https://drive.google.com/drive/folders/15rWyik07PbCrXqJzc2oMEPlqLQXhiNbx

* Run the Script
From the project's root directory, execute the runme.sh script and provide the path to the folder where you saved the datasets.

    ./runme.sh [path_to_the_datasets_folder]
    
The script will run all the tests and generate a file called results_table.md in the same directory, which contains the results in a table similar to the one in the paper.



